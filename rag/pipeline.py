#Kh·ªëi import
import json
import re
from datetime import datetime
from rag.config import RAGConfig
from rag.router import route_query
from rag.normalize import normalize_query
from rag.text_utils import is_listing_query
from rag.retriever import search as retrieve_search
from rag.scoring import fused_score
from rag.context_builder import choose_adaptive_max_ctx, build_context_from_hits
from rag.memory.conversation_manager import read_memory, log_event, build_conversation_text, write_memory
from rag.memory.summarizer import summarize_to_fact
from rag.conversation_state import conversation_state
from rag.query_rewriter import needs_rewrite, format_history, rewrite_query_with_llm
from rag.answer_modes import decide_answer_policy
from rag.generator import call_finetune_with_context_stream, call_finetune_with_context, l3_draft_fast_from_kb
from rag.tag_filter import tag_filter_pipeline
from rag.logging.timing_logger import TimingLog
from rag.logging.debug_log import set_debug_dir
from rag.reasoning.multi_hop import multi_hop_controller
from typing import List, Tuple, Dict, Any
from rag.logging.debug_log import debug_log
from rag.logging.multi_query_logger import _safe_folder_name
from httpx import RemoteProtocolError
from rag.logging.logger_csv import append_log_to_csv
from rag.post_answer.solution_completion import run_solution_completion
from rag.post_answer.l3_gap_detector import detect_l3_gaps
from rag.post_answer.t5_knowledge_fallback import t5_knowledge_fallback
# from rag.post_answer.enricher import enrich_answer_if_needed
from pathlib import Path

def make_run_dir(query: str):
    base = Path("debug_runs")
    base.mkdir(exist_ok=True)

    ts = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    name = _safe_folder_name(query)

    root = base / f"{ts}__{name}"
    root.mkdir(parents=True, exist_ok=True)

    return root

def emit_trace_snapshot(*,
    user_query,
    effective_query,
    norm_query,
    must_tags,
    any_tags,
    hits,
    base_ctx,
    context,
    l3_missing_slots,
    missing_after_t4,
    t4_report,
    need_kb_fallback,
    memory_prompt,
    final_system_override,
    answer_mode,
):
    def safe_len(x):
        try:
            return len(x)
        except:
            return 0

    print("\n================ FINAL RAG TRACE =================")
    print("USER QUERY      :", user_query)
    print("EFFECTIVE QUERY :", effective_query)
    print("NORMALIZED      :", norm_query)
    print("ANSWER MODE     :", answer_mode)
    print("--------------------------------------------------")

    print("TAGS:")
    print("  MUST :", must_tags)
    print("  ANY  :", any_tags)
    print("--------------------------------------------------")

    print("RETRIEVAL:")
    print("  total hits :", len(hits))
    print("  T4 hits    :", sum(1 for h in hits if h.get("t4_origin_query")))
    print("  KB hits    :", sum(1 for h in hits if not h.get("t4_origin_query")))
    print("--------------------------------------------------")

    print("L3 / T4:")
    print("  L3 missing slots      :", l3_missing_slots)
    print("  missing after T4     :", missing_after_t4)
    print("  T4 report present    :", bool(t4_report))
    print("--------------------------------------------------")

    print("T5 FALLBACK:")
    print("  need_kb_fallback :", need_kb_fallback)
    if need_kb_fallback:
        print("  [T5] knowledge injected")
    else:
        print("  [T5] NOT triggered")
    print("--------------------------------------------------")

    print("CONTEXT:")
    print("  base ctx length :", safe_len(base_ctx))
    print("  final ctx length:", safe_len(context))
    print("  T5 added chars  :", max(0, safe_len(context) - safe_len(base_ctx)))
    print("--------------------------------------------------")

    print("SYSTEM PROMPT:")
    print("  memory chars   :", safe_len(memory_prompt))
    print("  override chars :", safe_len(final_system_override))
    print("--------------------------------------------------")

    print("MODEL INPUT SUMMARY:")
    print("  system total   :", safe_len(memory_prompt + final_system_override))
    print("  context total  :", safe_len(context))
    print("==================================================\n")


KB_FALLBACK_SLOTS = {
    "need_pesticide",
    "need_foliar_fertilizer",
    "need_mix_compatibility",
    "need_dosage_or_rate",
    "need_timing",
    "need_crop",
    "need_pest_or_disease",
    "need_general_knowledge",
}

FORCE_MUST_TAGS = {
    "mechanisms:luu-dan-manh",
    "mechanisms:luu-dan",
    "mechanisms:tiep-xuc-manh",
    "mechanisms:tiep-xuc",
    "mechanisms:tiep-xuc-luu-dan-manh",
    "mechanisms:tiep-xuc-luu-dan",
    "mechanisms:xong-hoi-manh",
    "mechanisms:xong-hoi",
    "mechanisms:co-chon-loc",
    "mechanisms:khong-chon-loc",
}

FORMULA_TRIGGERS = [
    
    "c√¥ng th·ª©c",
    "ph·ªëi tr·ªôn",
    "ph·ªëi h·ª£p thu·ªëc",
    "li·ªÅu ph·ªëi",
    "ph·ªëi",
    "pha thu·ªëc",
    "c√¥ng th·ª©c tr·ªã",
    "c√¥ng th·ª©c tr·ª´",
    "c√¥ng th·ª©c di·ªát",
    "ph√°c ƒë·ªì",
    "k·∫øt h·ª£p thu·ªëc",
    "ho·∫°t ch·∫•t l∆∞u d·∫´n ph√π h·ª£p",
]

#chu·∫©n h√≥a c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
def norm(s: str) -> str:
    if not s:
        return ""
    s = s.lower()
    s = s.replace("-", " ").replace("_", " ")
    s = re.sub(r"[^\w\s]", " ", s)
    s = re.sub(r"\s+", " ", s)
    return s.strip()

#Nh·∫≠n di·ªán truy v·∫•n d·∫°ng ph·ªëi c√¥ng th·ª©c
def is_formula_query(query: str, tags: dict) -> bool:

    has_plus = "+" in query
    has_mechanisms = any(
        t.startswith("mechanisms:")
        for t in tags.get("must", []) + tags.get("soft", [])
    )

    # N·∫øu c√≥ nhi·ªÅu h∆°n 1 mechanism tag -> g·∫ßn nh∆∞ ch·∫Øc ch·∫Øn l√† ph·ªëi
    num_mechs = sum(
        1 for t in tags.get("must", []) + tags.get("soft", [])
        if t.startswith("mechanisms:")
    )

    if has_mechanisms and (has_plus or num_mechs >= 2):
        return True

    return False

#T√¨m ki·∫øm theo ch·∫ø ƒë·ªô c√¥ng th·ª©c (kh√¥ng d√πng multi-hop)
def formula_mode_search(
    *,
    client,
    kb,
    norm_query: str,
    must_tags: List[str],
):
    """
    Budget-aware:
    - T·ªïng ng√¢n s√°ch = RAGConfig.max_ctx_soft
    - Chia ƒë·ªÅu cho m·ªói must_tag
    - N·∫øu ch∆∞a ƒë·ªß ‚Üí ch·∫°y free search ƒë·ªÉ b√π
    """

    max_ctx = RAGConfig.max_ctx_soft

    must_tags = list(must_tags or [])
    num_tags = max(len(must_tags), 1)

    # ng√¢n s√°ch cho m·ªói tag
    per_tag_k = max(1, max_ctx // num_tags)

    all_results = []

    # ---- ROLE 1: MUST TAG (chia ng√¢n s√°ch) ----
    for m in must_tags:
        print("m:", m)
        hits = retrieve_search(
            client=client,
            kb=kb,
            norm_query=norm_query,
            top_k=per_tag_k,
            must_tags=[m],
            any_tags=[]
        )
        all_results.extend(hits)

    # dedupe theo id
    unique = {}
    for h in all_results:
        hid = h.get("id")
        if hid:
            prev = unique.get(hid)
            if not prev or h.get("score", 0) > prev.get("score", 0):
                unique[hid] = h

    results = list(unique.values())

    # ---- ROLE 2: FREE SEARCH (b√π slot c√≤n thi·∫øu) ----
    remaining = max_ctx - len(results)

    if remaining > 0:
        hits_free = retrieve_search(
            client=client,
            kb=kb,
            norm_query=norm_query,
            top_k=remaining,
            must_tags=[],
            any_tags=[]
        )

        for h in hits_free:
            hid = h.get("id")
            if hid and hid not in unique:
                unique[hid] = h
                if len(unique) >= max_ctx:
                    break

        results = list(unique.values())

    # hard cap an to√†n
    return results[:max_ctx]

#Chu·∫©n h√≥a tag
def strip_tag_ns(s):
    if not s:
        return ""
    out = []
    for part in s.split("|"):
        if ":" in part:
            out.append(part.split(":",1)[1])
        else:
            out.append(part)
    return " ".join(out)

#Chu·∫©n h√≥a tag
def strip_ns(t):
    if ":" in t:
        t = t.split(":",1)[1]
    return t

#Filter DOC match tags/keyword
def evidence_gate_by_tags(hits, must_tags, any_tags):
    must_tags = [norm(strip_ns(t)) for t in must_tags]
    any_tags  = [norm(strip_ns(t)) for t in any_tags]

    kept = []

    for h in hits:
        blob = norm(
            (h.get("question","") or "") + " " +
            (h.get("answer","") or "") + " " +
            " ".join(h.get("alt_question",[]) or []) + " " +
            strip_tag_ns(h.get("tags_v2",""))
        )

        must_hit = sum(1 for t in must_tags if t in blob)
        any_hit  = sum(1 for t in any_tags  if t in blob)

        if must_hit >= 1 or any_hit >= 1:
            kept.append((must_hit, any_hit, h))

    kept.sort(key=lambda x: (x[0], x[1]), reverse=True)

    return [h for _,_,h in kept[:RAGConfig.max_ctx_soft]]

#ƒê√°nh d·∫•u th·ª© t·ª± g·ªëc t·ª´ search() ƒë·ªÉ pipeline KH√îNG l√†m x√°o tr·ªôn
def preserve_search_order(hits):
    for idx, h in enumerate(hits):
        h["_search_rank"] = idx
    return hits

#C·ªông ƒëi·ªÉm tag match
def _count_tag_hits(h, any_tags, must_tags):
    tv2 = str(h.get("tags_v2") or "")
    score = 0
    for t in (must_tags or []):
        if t and t in tv2:
            score += 3
    for t in (any_tags or []):
        if t and t in tv2:
            score += 1
    return score

#Promt ph·ª•c v·ª• ƒëi nh√°nh GLOBAL
def _global_system_prompt() -> str:
    return """
B·∫°n l√† chuy√™n gia BVTV/n√¥ng h·ªçc t·∫°i Vi·ªát Nam. M·ª•c ti√™u: cung c·∫•p c√¢u tr·∫£ l·ªùi CH·∫§T L∆Ø·ª¢NG CAO theo phong c√°ch gi√°o tr√¨nh/chuy√™n kh·∫£o,
gi·∫£i th√≠ch r√µ r√†ng, c√≥ chi·ªÅu s√¢u, gi√†u v√≠ d·ª• th·ª±c t·∫ø trong canh t√°c Vi·ªát Nam.

TI√äU CHU·∫®N CH·∫§T L∆Ø·ª¢NG (B·∫ÆT BU·ªòC):
- ∆Øu ti√™n: ch√≠nh x√°c, m·∫°ch l·∫°c, c√≥ t√≠nh ‚Äúgi·∫£i th√≠ch ƒë∆∞·ª£c‚Äù (explainable), kh√¥ng n√≥i chung chung.
- Tr√¨nh b√†y theo c·∫•u tr√∫c r√µ r√†ng, c√≥ ti√™u ƒë·ªÅ; d√πng bullet v√† b·∫£ng (n·∫øu h·ªØu √≠ch).
- Lu√¥n ph√¢n bi·ªát: (i) ƒëi·ªÅu ch·∫Øc ch·∫Øn/ph·ªï qu√°t, (ii) ƒëi·ªÅu ph·ª• thu·ªôc b·ªëi c·∫£nh (c√¢y, giai ƒëo·∫°n, th·ªùi ti·∫øt, √°p l·ª±c d·ªãch h·∫°i), (iii) ƒëi·ªÅu c·∫ßn th√™m d·ªØ li·ªáu.
- Khi thu·∫≠t ng·ªØ/ƒë·ªëi t∆∞·ª£ng c√≥ nhi·ªÅu c√°ch g·ªçi t·∫°i VN: n√™u t√™n th∆∞·ªùng g·ªçi + m√¥ t·∫£ nh·∫≠n di·ªán; tr√°nh b·ªãa t√™n lo√†i.
- N·∫øu thi·∫øu d·ªØ li·ªáu ƒë·ªÉ k·∫øt lu·∫≠n ch·∫Øc: n√≥i r√µ ‚Äúph·ª• thu·ªôc/ c·∫ßn x√°c minh‚Äù v√† ƒë∆∞a ti√™u ch√≠/quan s√°t ƒë·ªÉ ng∆∞·ªùi d√πng t·ª± ki·ªÉm ch·ª©ng.

C·∫§U TR√öC C√ÇU TR·∫¢ L·ªúI CHU·∫®N:
1) T√≥m t·∫Øt nhanh (2‚Äì4 d√≤ng): tr·∫£ l·ªùi tr·ª±c di·ªán c√¢u h·ªèi.
2) ƒê·ªãnh nghƒ©a/kh√°i ni·ªám c·ªët l√µi (ng·∫Øn g·ªçn).
3) ƒê·∫∑c ƒëi·ªÉm nh·∫≠n bi·∫øt / ƒëi·ªÉm then ch·ªët (3‚Äì7 bullet).
4) C∆° ch·∫ø / nguy√™n l√Ω (n·∫øu li√™n quan): gi·∫£i th√≠ch ·ªü m·ª©c v·ª´a ƒë·ªß, tr√°nh thu·∫≠t ng·ªØ qu√° h√†n l√¢m nh∆∞ng ph·∫£i ƒë√∫ng.
5) Ph√¢n lo·∫°i (CH·ªà KHI c√¢u h·ªèi h·ªèi ‚Äúg·ªìm nh·ªØng lo·∫°i n√†o/bao g·ªìm/ph√¢n lo·∫°i‚Äù): k√®m ti√™u ch√≠ ph√¢n bi·ªát.
6) V√≠ d·ª• ƒë·∫°i di·ªán: ∆∞u ti√™n nh√≥m/case ph·ªï bi·∫øn trong canh t√°c Vi·ªát Nam (n√™u 3‚Äì8 v√≠ d·ª• ph√π h·ª£p).
7) Sai l·∫ßm th∆∞·ªùng g·∫∑p & c√°ch tr√°nh (2‚Äì5 √Ω) ‚Äî ch·ªâ n√™u khi gi√∫p √≠ch tr·ª±c ti·∫øp.
8) C√¢u h·ªèi c·∫ßn l√†m r√µ (2‚Äì6 c√¢u): ƒë·ªÉ ch·ªët quy·∫øt ƒë·ªãnh th·ª±c t·∫ø theo b·ªëi c·∫£nh ng∆∞·ªùi d√πng.

QUY T·∫ÆC TR·∫¢ L·ªúI:
- Kh√¥ng lan man sang ch·ªß ƒë·ªÅ ngo√†i tr·ªçng t√¢m c√¢u h·ªèi.
- Kh√¥ng ‚Äút·ªè ra ch·∫Øc ch·∫Øn‚Äù khi thi·∫øu c∆° s·ªü; kh√¥ng suy di·ªÖn v∆∞·ª£t qu√° th√¥ng tin ƒë·∫ßu v√†o.
- D√πng thu·∫≠t ng·ªØ BVTV quen thu·ªôc t·∫°i Vi·ªát Nam; n·∫øu d√πng thu·∫≠t ng·ªØ qu·ªëc t·∫ø th√¨ gi·∫£i th√≠ch ng·∫Øn k√®m theo.
- VƒÉn phong chuy√™n nghi·ªáp, d·ªÖ hi·ªÉu; ∆∞u ti√™n v√≠ d·ª• v√† ti√™u ch√≠ ph√¢n bi·ªát h∆°n l√† l√Ω thuy·∫øt d√†i d√≤ng.
""".strip()

#Core GPT
def answer_with_suggestions_stream(*, user_id, user_query, kb, client, cfg, policy):
    timer = TimingLog(user_query)
    run_dir = make_run_dir(user_query)
    set_debug_dir(run_dir)

    turns = conversation_state.get_turns(user_id)
    effective_query = user_query

    # 1) rewrite
    if turns and needs_rewrite(user_query):
        history_text = format_history(turns)
        try:
            rewritten = rewrite_query_with_llm(
                client=client,
                user_query=user_query,
                history_text=history_text
            )
            if rewritten:
                effective_query = rewritten
        except Exception as e:
            print("[QUERY REWRITE ERROR]:", e)

    # 2) route + normalize
    route = route_query(client, effective_query)
    if route == "GLOBAL":
        model = "gpt-4.1"
        yield "üåç ƒêang tr·∫£ l·ªùi b·∫±ng tri th·ª©c t·ªïng qu√°t...\n\n"

        resp = client.chat.completions.create(
            model=model,
            temperature=0.25,
            messages=[
                {"role": "system", "content": _global_system_prompt()},
                {"role": "user", "content": effective_query},
            ],
            stream=True,
        )

        parts = []
        for chunk in resp:
            try:
                delta = chunk.choices[0].delta
                if delta and getattr(delta, "content", None):
                    parts.append(delta.content)
                    yield delta.content
            except Exception:
                continue

        final_text = "".join(parts)

        conversation_state.append(user_id, "user", user_query)
        conversation_state.append(user_id, "assistant", final_text)
        log_event(user_id, "user", user_query)
        log_event(user_id, "assistant", final_text)

        timer.finish(RAGConfig.enable_timing_log)
        return

    timer.start("normalize")
    norm_query = normalize_query(client, effective_query)
    norm_lower = norm_query.lower()
    timer.end("normalize")

    # 3) read memory + force_rag
    timer.start("read_memory and check route")
    memory_facts = read_memory(client=client, user_id=user_id, query=norm_query)
    memory_prompt = ""
    if memory_facts:
        memory_prompt = "USER MEMORY:\n" + "\n".join(f"- {m['fact']}" for m in memory_facts)

    force_rag = any(k in norm_lower for k in FORMULA_TRIGGERS)
    if force_rag:
        route = "RAG"
    timer.end("read_memory and check route")

    # UI status
    yield "‚è≥ ƒêang truy v·∫•n d·ªØ li·ªáu...\n\n"

    # 4) tag filter
    timer.start("tag_filter_pipeline running")
    result = tag_filter_pipeline(norm_query)
    must_tags = result.get("must", [])
    any_tags  = result.get("any", [])
    timer.end("tag_filter_pipeline running")

    # 5) retrieval
    timer.start("retrieval")
    is_list = is_listing_query(norm_query)
    if is_formula_query(norm_query, {"must": must_tags, "soft": any_tags}):
        hits = formula_mode_search(
            client=client,
            kb=kb,
            norm_query=norm_query,
            must_tags=must_tags
        )
    else:
        hits = multi_hop_controller(
            client=client,
            kb=kb,
            base_query=norm_query,
            must_tags=must_tags,
            any_tags=any_tags,
        )
    timer.end("retrieval")

    print("QUERY      :", norm_query)
    print("MUST TAGS  :", must_tags)
    print("ANY TAGS   :", any_tags)
    debug_log("QUERY      :", norm_query)
    debug_log("MUST TAGS  :", must_tags)
    debug_log("ANY TAGS   :", any_tags)

    if not hits:
        yield "Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ph√π h·ª£p."
        return

    # fused score, tag hits, ordering
    for h in hits:
        h["fused_score"] = fused_score(h)
        h["tag_hits"] = _count_tag_hits(h, any_tags, must_tags)

    hits = preserve_search_order(hits)
    primary_doc = hits[0]

    # 6) build context (pre T3/T4/T5)
    timer.start("build_context")
    base_ctx = choose_adaptive_max_ctx(hits, is_listing=is_list)
    max_ctx = min(RAGConfig.max_ctx_strict, base_ctx)

    policy = decide_answer_policy(effective_query, primary_doc, force_listing=is_list)

    off_filter_tag_on_doc = policy.intent not in {"disease"}
    if not off_filter_tag_on_doc:
        base_hits = [h for h in hits if not h.get("t4_origin_query")]
        t4_hits   = [h for h in hits if h.get("t4_origin_query")]
        base_hits = evidence_gate_by_tags(base_hits, must_tags=must_tags, any_tags=any_tags)
        hits = t4_hits + base_hits

    context = build_context_from_hits(hits[:max_ctx])
    timer.end("build_context")

    # ===========================
    # L3 ‚Äî KB Gap Detector
    # ===========================
    t4_report = None

    timer.start("l3_draft")
    # IMPORTANT: draft trung th·ª±c t·ª´ KB (kh√¥ng d√πng LLM ƒë·ªÉ tr√°nh che l·ªó h·ªïng KB)
    kb_draft = l3_draft_fast_from_kb(hits)
    timer.end("l3_draft")

    timer.start("l3_gap")
    gap = detect_l3_gaps(client, norm_query, kb_draft)
    timer.end("l3_gap")

    l3_missing_slots = gap.get("missing_slots", []) or []

    # ===========================
    # T4 ‚Äî Solution Completion
    # ===========================
    if l3_missing_slots:
        timer.start("t4_retrieval")
        # NOTE: run_solution_completion ph·∫£i return (hits, t4_report)
        hits, t4_report = run_solution_completion(
            run_dir=run_dir,
            client=client,
            kb=kb,
            user_query=norm_query,
            hits=hits,
            must_tags=must_tags,
            any_tags=any_tags,
            l3_missing_slots=l3_missing_slots,
        )
        timer.end("t4_retrieval")

        # ∆∞u ti√™n doc T4
        hits.sort(key=lambda h: 1 if h.get("t4_origin_query") else 0, reverse=True)

        timer.start("build_context_t4")
        context = build_context_from_hits(hits[:max_ctx])
        timer.end("build_context_t4")

    # ===========================
    # T5 ‚Äî Knowledge Fallback
    # ===========================
    # 1) N·∫øu T4 report n√≥i KB kh√¥ng c√≥ solution ‚Üí b·∫≠t T5
    # 2) Ho·∫∑c n·∫øu L3 c√≥ need_general_knowledge m√† T4_report None (case bypass) ‚Üí c≈©ng b·∫≠t T5
    need_kb_fallback = False

    missing_after_t4 = []
    if t4_report is not None:
        raw = t4_report.get("l3_missing_slots") or t4_report.get("missing_slots") or []
        missing_after_t4 = raw if isinstance(raw, list) else []
    else:
        missing_after_t4 = l3_missing_slots if isinstance(l3_missing_slots, list) else []

    # Ch·ªâ c·∫ßn c√≤n need_general_knowledge l√† b·∫≠t T5
    need_kb_fallback = any(
        slot in KB_FALLBACK_SLOTS
        for slot in missing_after_t4
    )

    final_system_override = ""
    if need_kb_fallback:
        timer.start("t5_fallback")
        kb_fallback_text = t5_knowledge_fallback(
            client=client,
            user_query=norm_query,
            missing_slots=missing_after_t4,
            context=context,
        )
        context += "\n\n[KI·∫æN TH·ª®C N·ªÄN]\n" + (kb_fallback_text or "")
        timer.end("t5_fallback")
        final_system_override = """
        ‚ö†Ô∏è INTERNAL DATA IS INSUFFICIENT.

        You MUST use the [KI·∫æN TH·ª®C N·ªÄN] section
        to complete the user's objective.
        Do not answer using only internal data.
        """
    yield "‚úçÔ∏è ƒêang t·ªïng h·ª£p c√¢u tr·∫£ l·ªùi...\n\n"

    # 7) generate streaming (FINAL)
    answer_mode_final = (
        "formula" if is_formula_query(norm_query, {"must": must_tags, "soft": any_tags}) else "default"
    )

    emit_trace_snapshot(
        user_query=user_query,
        effective_query=effective_query,
        norm_query=norm_query,
        must_tags=must_tags,
        any_tags=any_tags,
        hits=hits,
        base_ctx=build_context_from_hits(hits[:max_ctx]),  # context tr∆∞·ªõc T4/T5
        context=context,
        l3_missing_slots=l3_missing_slots,
        missing_after_t4=missing_after_t4,
        t4_report=t4_report,
        need_kb_fallback=need_kb_fallback,
        memory_prompt=memory_prompt,
        final_system_override=final_system_override,
        answer_mode=answer_mode_final,
    )

    timer.start("final_gpt_ttft")
    timer.start("final_gpt_total")
    first_tok = True
    parts = []
    stream_failed = False

    try:
        for tok in call_finetune_with_context_stream(
            system_prefix=memory_prompt + final_system_override,
            client=client,
            user_query=effective_query,
            context=context,  # context ƒë√£ c√≥ T4/T5 n·∫øu c√≥
            answer_mode=answer_mode_final,
            must_tags=must_tags,
            any_tags=any_tags,
        ):
            if first_tok:
                timer.end("final_gpt_ttft")
                first_tok = False

            parts.append(tok)
            yield tok

    except RemoteProtocolError as e:
        print("[STREAM DROPPED] OpenAI closed connection mid-stream:", e)
        stream_failed = True

    except Exception as e:
        print("[STREAM ERROR] Unknown streaming error:", e)
        stream_failed = True

    finally:
        # lu√¥n end t·ªïng th·ªùi gian stream
        timer.end("final_gpt_total")

    # -------------------------------------------------
    # Fallback: re-run in NON-STREAM mode if stream died
    # -------------------------------------------------
    if stream_failed:
        print("[STREAM RECOVERY] Re-running in non-stream mode")
        try:
            final_answer = call_finetune_with_context(
                client=client,
                user_query=user_query,
                context=context,
                answer_mode=answer_mode_final,
                rag_mode="STRICT",
            )
            if parts:
                yield "\n\n[‚ö† K·∫øt n·ªëi b·ªã gi√°n ƒëo·∫°n ‚Äì ti·∫øp t·ª•c k·∫øt qu·∫£ ƒë·∫ßy ƒë·ªß]\n\n"
            yield final_answer
            final_answer = "".join(parts) + final_answer
        except Exception as e:
            print("[STREAM RECOVERY FAILED]:", e)
            final_answer = "".join(parts)
    else:
        final_answer = "".join(parts)

    # 8) log CSV
    try:
        append_log_to_csv(
            run_dir,
            user_query,
            norm_query,
            context,
            {
                "text": final_answer,
                "route": route,
                "norm_query": norm_query,
                "missing_slots": l3_missing_slots,
            },
            route
        )
    except Exception as e:
        print("[RAG CSV LOG ERROR]:", e)

    # 9) memory/log
    conversation_state.append(user_id, "user", user_query)
    conversation_state.append(user_id, "assistant", final_answer)
    log_event(user_id, "user", user_query)
    log_event(user_id, "assistant", final_answer)

    try:
        conv_text = build_conversation_text(user_id)
        facts_raw = summarize_to_fact(client, conv_text)
        facts = json.loads(facts_raw)
        write_memory(client, user_id, facts)
    except Exception as e:
        print("[MEMORY WRITE ERROR]:", e)

    timer.finish(RAGConfig.enable_timing_log)
